{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于策略方法\n",
    "\n",
    "q-learning和DQN都是基于价值的方法，目标是训练价值函数，agent通过输入场景和操作获取最高价值的操作结果来进行行动。训练过程本质是：减少真实q-value和预测的q-value的loss，使得agent预测q-value更准。\n",
    "\n",
    "基于策略的方法跳过训练价值函数。通过参数化策略，如直接使用一个神经网络作为agent，输入场景，输出操作概率分布\n",
    "\n",
    "## 实现\n",
    "\n",
    "通过定义函数J(theta)代表累积的奖励，theta是模型参数，目标是找到一个theta最大化函数\n",
    "\n",
    "## 策略梯度方法\n",
    "\n",
    "基于策略和策略梯度方法之间的差异在于：策略梯度方法是基于策略方法的一个子类。在基于策略的方法中，大多数时候优化是在策略上进行的，因为每次更新仅使用由最新版本的策略参数化函数收集的数据（轨迹）。这两种方法的区别在于优化参数的方式：在基于策略的方法中，直接寻找最优策略，通过爬山、模拟退火或进化策略等技术最大化目标函数的局部近似来间接优化参数；在策略梯度方法中，同样直接寻找最优策略，但通过对目标函数的性能执行梯度上升来直接优化参数。\n",
    "\n",
    "# actor-critic method\n",
    "\n",
    "actor-critic 是一种强化学习方法，它结合了基于价值的方法和基于策略的方法。其目的是通过学习一个演员（actor）策略和一个评论家（critic）价值函数来优化策略以获得最大累积奖励。演员根据策略选择动作，评论家评估当前状态的价值，为演员提供反馈以改进策略"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
