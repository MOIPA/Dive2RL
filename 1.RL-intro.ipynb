{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习算法\n",
    "\n",
    "强化学习本质是训练一个agent，存在一个environment，agent输出操作给环境，环境执行后返回状态和奖励，agent接受后生成新的操作\n",
    "\n",
    "比如：降落游戏，agent是飞船，有四种输出，每种对应控制飞船推进器，环境是一个雅达利游戏，接受操作控制飞船，如果飞船降落给100分，如果坠毁或者其他奖励为负数\n",
    "\n",
    "目标就是训练这个agent，通过这种奖励机制，使得能够完成降落\n",
    "\n",
    "## 强化学习工作类型\n",
    "\n",
    "两种：\n",
    "\n",
    "1. 关卡式的，比如飞船降落，每次坠毁就代表关卡结束\n",
    "2. 连续式的，比如操作股票，没有停止的时候\n",
    "\n",
    "## 强化学习算法/训练类型\n",
    "\n",
    "agent有两种：\n",
    "\n",
    "1. 基于价值函数的，输入：当前这个状态+不同操作，输出未来价值最高的操作\n",
    "2. 确定性的，输入：当前状态，输出操作，或者操作的概率分布（a操作：80%，b操作20%）\n",
    "\n",
    "对应的agent的训练也有两种：\n",
    "\n",
    "1. 基于策略训练：直接训练策略，底层可能是个MLP，输入状态，输出行为\n",
    "2. 基于价值的训练：训练的是价值函数，找到一个函数，初始可能啥都不知道，训练完了以后，输入环境和操作，输出价值\n",
    "\n",
    "## 深度强化学习算法\n",
    "\n",
    "基于神经网络来做RL\n",
    "\n",
    "而且一般训练的时候，游戏环境是一次开多个游戏，每个游戏的状态（或者观察）压缩在一个向量\n",
    "\n",
    "环境有很多种：\n",
    "\n",
    "    1. mlagents 是可以不开启渲染的情况下和unity游戏交互\n",
    "    2. gymnasium 提供了各种游戏环境，可以不开启渲染\n",
    "\n",
    "agent也有库有实现：\n",
    "\n",
    "    1. stable_baselines3 实现了各种模型算法，包括PPO等\n",
    "\n",
    "模型和hf的库，用于上传模型：\n",
    "\n",
    "    1. huggingface_hub\n",
    "    2. huggingface_sb3：专门加载和保存sb3的算法模型\n",
    "\n",
    "基于价值函数的算法：Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install swig cmake\n",
    "!pip install shimmy\n",
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt\n",
    "!sudo apt-get update\n",
    "!apt install python3-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用noetbook时，配置好后要重启内核\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import (\n",
    "    notebook_login,\n",
    ")  # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境库小demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    print(\"Action taken:\", action)\n",
    "\n",
    "    # Do this action in the environment and get\n",
    "    # next_state, reward, terminated, truncated and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "    if terminated or truncated:\n",
    "        # Reset the environment\n",
    "        print(\"Environment is reset\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建场景并且查看在这个游戏下，环境是什么样的，操作有哪些取值\n",
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n",
    "\n",
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如何一次创建n个场景，并且将这些场景合并成一个向量\n",
    "# action的形状通常是(16, act_dim)（act_dim是动作空间的维度），表示每个环境执行的动作。\n",
    "# 例如，如果动作是控制飞船推进器的开关（假设动作空间维度act_dim = 4），那么action就是一个形状为(16, 4)的数组，用于同时对 16 个环境中的飞船进行控制\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "# 创建demo真正的游戏场景，飞船降落\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "# 创建使用PPO算法的agent\n",
    "# We added some parameters to accelerate the training\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# 训练agent\n",
    "# SOLUTION\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "model.save(model_name)\n",
    "\n",
    "# 评估agent\n",
    "# @title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上传模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登陆自己的hf账号，得到访问的key (https://huggingface.co/settings/tokens) \n",
    "# 如果是notebook情况下\n",
    "notebook_login()\n",
    "!git config --global credential.helper store\n",
    "\n",
    "# 如果是其他情况，在命令行执行：huggingface-cli login\n",
    "\n",
    "# 执行上传\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"Segment139/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine 😄\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(\n",
    "    model=model,  # Our trained model\n",
    "    model_name=model_name,  # The name of our trained model\n",
    "    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n",
    "    env_id=env_id,  # Name of the environment\n",
    "    eval_env=eval_env,  # Evaluation Environment\n",
    "    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "    commit_message=commit_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载其他人的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "\n",
    "repo_id = \"Classroom-workshop/assignment2-omar\"  # The repo_id\n",
    "filename = \"ppo-LunarLander-v2.zip\"  # The model filename.zip\n",
    "\n",
    "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
    "# But Python 3.6, 3.7 use protocol 4\n",
    "# In order to get compatibility we need to:\n",
    "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
    "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
    "custom_objects = {\n",
    "    \"learning_rate\": 0.0,\n",
    "    \"lr_schedule\": lambda _: 0.0,\n",
    "    \"clip_range\": lambda _: 0.0,\n",
    "}\n",
    "\n",
    "checkpoint = load_from_hub(repo_id, filename)\n",
    "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)\n",
    "\n",
    "# 评估模型\n",
    "# @title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
