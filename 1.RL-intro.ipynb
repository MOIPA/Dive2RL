{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¼ºåŒ–å­¦ä¹ ç®—æ³•\n",
    "\n",
    "å¼ºåŒ–å­¦ä¹ æœ¬è´¨æ˜¯è®­ç»ƒä¸€ä¸ªagentï¼Œå­˜åœ¨ä¸€ä¸ªenvironmentï¼Œagentè¾“å‡ºæ“ä½œç»™ç¯å¢ƒï¼Œç¯å¢ƒæ‰§è¡Œåè¿”å›çŠ¶æ€å’Œå¥–åŠ±ï¼Œagentæ¥å—åç”Ÿæˆæ–°çš„æ“ä½œ\n",
    "\n",
    "æ¯”å¦‚ï¼šé™è½æ¸¸æˆï¼Œagentæ˜¯é£èˆ¹ï¼Œæœ‰å››ç§è¾“å‡ºï¼Œæ¯ç§å¯¹åº”æ§åˆ¶é£èˆ¹æ¨è¿›å™¨ï¼Œç¯å¢ƒæ˜¯ä¸€ä¸ªé›…è¾¾åˆ©æ¸¸æˆï¼Œæ¥å—æ“ä½œæ§åˆ¶é£èˆ¹ï¼Œå¦‚æœé£èˆ¹é™è½ç»™100åˆ†ï¼Œå¦‚æœå æ¯æˆ–è€…å…¶ä»–å¥–åŠ±ä¸ºè´Ÿæ•°\n",
    "\n",
    "ç›®æ ‡å°±æ˜¯è®­ç»ƒè¿™ä¸ªagentï¼Œé€šè¿‡è¿™ç§å¥–åŠ±æœºåˆ¶ï¼Œä½¿å¾—èƒ½å¤Ÿå®Œæˆé™è½\n",
    "\n",
    "## å¼ºåŒ–å­¦ä¹ å·¥ä½œç±»å‹\n",
    "\n",
    "ä¸¤ç§ï¼š\n",
    "\n",
    "1. å…³å¡å¼çš„ï¼Œæ¯”å¦‚é£èˆ¹é™è½ï¼Œæ¯æ¬¡å æ¯å°±ä»£è¡¨å…³å¡ç»“æŸ\n",
    "2. è¿ç»­å¼çš„ï¼Œæ¯”å¦‚æ“ä½œè‚¡ç¥¨ï¼Œæ²¡æœ‰åœæ­¢çš„æ—¶å€™\n",
    "\n",
    "## å¼ºåŒ–å­¦ä¹ ç®—æ³•/è®­ç»ƒç±»å‹\n",
    "\n",
    "agentæœ‰ä¸¤ç§ï¼š\n",
    "\n",
    "1. åŸºäºä»·å€¼å‡½æ•°çš„ï¼Œè¾“å…¥ï¼šå½“å‰è¿™ä¸ªçŠ¶æ€+ä¸åŒæ“ä½œï¼Œè¾“å‡ºæœªæ¥ä»·å€¼æœ€é«˜çš„æ“ä½œ\n",
    "2. ç¡®å®šæ€§çš„ï¼Œè¾“å…¥ï¼šå½“å‰çŠ¶æ€ï¼Œè¾“å‡ºæ“ä½œï¼Œæˆ–è€…æ“ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼ˆaæ“ä½œï¼š80%ï¼Œbæ“ä½œ20%ï¼‰\n",
    "\n",
    "å¯¹åº”çš„agentçš„è®­ç»ƒä¹Ÿæœ‰ä¸¤ç§ï¼š\n",
    "\n",
    "1. åŸºäºç­–ç•¥è®­ç»ƒï¼šç›´æ¥è®­ç»ƒç­–ç•¥ï¼Œåº•å±‚å¯èƒ½æ˜¯ä¸ªMLPï¼Œè¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºè¡Œä¸º\n",
    "2. åŸºäºä»·å€¼çš„è®­ç»ƒï¼šè®­ç»ƒçš„æ˜¯ä»·å€¼å‡½æ•°ï¼Œæ‰¾åˆ°ä¸€ä¸ªå‡½æ•°ï¼Œåˆå§‹å¯èƒ½å•¥éƒ½ä¸çŸ¥é“ï¼Œè®­ç»ƒå®Œäº†ä»¥åï¼Œè¾“å…¥ç¯å¢ƒå’Œæ“ä½œï¼Œè¾“å‡ºä»·å€¼\n",
    "\n",
    "## æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•\n",
    "\n",
    "åŸºäºç¥ç»ç½‘ç»œæ¥åšRL\n",
    "\n",
    "è€Œä¸”ä¸€èˆ¬è®­ç»ƒçš„æ—¶å€™ï¼Œæ¸¸æˆç¯å¢ƒæ˜¯ä¸€æ¬¡å¼€å¤šä¸ªæ¸¸æˆï¼Œæ¯ä¸ªæ¸¸æˆçš„çŠ¶æ€ï¼ˆæˆ–è€…è§‚å¯Ÿï¼‰å‹ç¼©åœ¨ä¸€ä¸ªå‘é‡\n",
    "\n",
    "ç¯å¢ƒæœ‰å¾ˆå¤šç§ï¼š\n",
    "\n",
    "    1. mlagents æ˜¯å¯ä»¥ä¸å¼€å¯æ¸²æŸ“çš„æƒ…å†µä¸‹å’Œunityæ¸¸æˆäº¤äº’\n",
    "    2. gymnasium æä¾›äº†å„ç§æ¸¸æˆç¯å¢ƒï¼Œå¯ä»¥ä¸å¼€å¯æ¸²æŸ“\n",
    "\n",
    "agentä¹Ÿæœ‰åº“æœ‰å®ç°ï¼š\n",
    "\n",
    "    1. stable_baselines3 å®ç°äº†å„ç§æ¨¡å‹ç®—æ³•ï¼ŒåŒ…æ‹¬PPOç­‰\n",
    "\n",
    "æ¨¡å‹å’Œhfçš„åº“ï¼Œç”¨äºä¸Šä¼ æ¨¡å‹ï¼š\n",
    "\n",
    "    1. huggingface_hub\n",
    "    2. huggingface_sb3ï¼šä¸“é—¨åŠ è½½å’Œä¿å­˜sb3çš„ç®—æ³•æ¨¡å‹\n",
    "\n",
    "åŸºäºä»·å€¼å‡½æ•°çš„ç®—æ³•ï¼šQ-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç®€å•DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é…ç½®ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install swig cmake\n",
    "!pip install shimmy\n",
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt\n",
    "!sudo apt-get update\n",
    "!apt install python3-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨noetbookæ—¶ï¼Œé…ç½®å¥½åè¦é‡å¯å†…æ ¸\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import (\n",
    "    notebook_login,\n",
    ")  # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒåº“å°demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    print(\"Action taken:\", action)\n",
    "\n",
    "    # Do this action in the environment and get\n",
    "    # next_state, reward, terminated, truncated and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "    if terminated or truncated:\n",
    "        # Reset the environment\n",
    "        print(\"Environment is reset\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåœºæ™¯å¹¶ä¸”æŸ¥çœ‹åœ¨è¿™ä¸ªæ¸¸æˆä¸‹ï¼Œç¯å¢ƒæ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæ“ä½œæœ‰å“ªäº›å–å€¼\n",
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n",
    "\n",
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚ä½•ä¸€æ¬¡åˆ›å»ºnä¸ªåœºæ™¯ï¼Œå¹¶ä¸”å°†è¿™äº›åœºæ™¯åˆå¹¶æˆä¸€ä¸ªå‘é‡\n",
    "# actionçš„å½¢çŠ¶é€šå¸¸æ˜¯(16, act_dim)ï¼ˆact_dimæ˜¯åŠ¨ä½œç©ºé—´çš„ç»´åº¦ï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªç¯å¢ƒæ‰§è¡Œçš„åŠ¨ä½œã€‚\n",
    "# ä¾‹å¦‚ï¼Œå¦‚æœåŠ¨ä½œæ˜¯æ§åˆ¶é£èˆ¹æ¨è¿›å™¨çš„å¼€å…³ï¼ˆå‡è®¾åŠ¨ä½œç©ºé—´ç»´åº¦act_dim = 4ï¼‰ï¼Œé‚£ä¹ˆactionå°±æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º(16, 4)çš„æ•°ç»„ï¼Œç”¨äºåŒæ—¶å¯¹ 16 ä¸ªç¯å¢ƒä¸­çš„é£èˆ¹è¿›è¡Œæ§åˆ¶\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "# åˆ›å»ºdemoçœŸæ­£çš„æ¸¸æˆåœºæ™¯ï¼Œé£èˆ¹é™è½\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "# åˆ›å»ºä½¿ç”¨PPOç®—æ³•çš„agent\n",
    "# We added some parameters to accelerate the training\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# è®­ç»ƒagent\n",
    "# SOLUTION\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "model.save(model_name)\n",
    "\n",
    "# è¯„ä¼°agent\n",
    "# @title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸Šä¼ æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç™»é™†è‡ªå·±çš„hfè´¦å·ï¼Œå¾—åˆ°è®¿é—®çš„key (https://huggingface.co/settings/tokens) \n",
    "# å¦‚æœæ˜¯notebookæƒ…å†µä¸‹\n",
    "notebook_login()\n",
    "!git config --global credential.helper store\n",
    "\n",
    "# å¦‚æœæ˜¯å…¶ä»–æƒ…å†µï¼Œåœ¨å‘½ä»¤è¡Œæ‰§è¡Œï¼šhuggingface-cli login\n",
    "\n",
    "# æ‰§è¡Œä¸Šä¼ \n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"Segment139/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine ğŸ˜„\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(\n",
    "    model=model,  # Our trained model\n",
    "    model_name=model_name,  # The name of our trained model\n",
    "    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n",
    "    env_id=env_id,  # Name of the environment\n",
    "    eval_env=eval_env,  # Evaluation Environment\n",
    "    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "    commit_message=commit_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½å…¶ä»–äººçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "\n",
    "repo_id = \"Classroom-workshop/assignment2-omar\"  # The repo_id\n",
    "filename = \"ppo-LunarLander-v2.zip\"  # The model filename.zip\n",
    "\n",
    "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
    "# But Python 3.6, 3.7 use protocol 4\n",
    "# In order to get compatibility we need to:\n",
    "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
    "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
    "custom_objects = {\n",
    "    \"learning_rate\": 0.0,\n",
    "    \"lr_schedule\": lambda _: 0.0,\n",
    "    \"clip_range\": lambda _: 0.0,\n",
    "}\n",
    "\n",
    "checkpoint = load_from_hub(repo_id, filename)\n",
    "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)\n",
    "\n",
    "# è¯„ä¼°æ¨¡å‹\n",
    "# @title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
